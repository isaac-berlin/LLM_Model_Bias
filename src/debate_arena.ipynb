{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelType(Enum):\n",
    "    PHI = \"phi\"\n",
    "    LLAMA = \"llama\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_path: str\n",
    "    model_type: ModelType\n",
    "    bias: str  # 'left' or 'right'\n",
    "    \n",
    "    @property\n",
    "    def friendly_name(self) -> str:\n",
    "        return f\"{self.model_type.value.title()}-{self.bias}\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_config(model_type: ModelType, bias: str) -> 'ModelConfig':\n",
    "        #base_path = \"/app/models\"\n",
    "        base_path = r\"C:\\Users\\isaac\\dev\\LLM_Model_Bias\\src\\models\"\n",
    "        \n",
    "        if model_type == ModelType.PHI:\n",
    "            return ModelConfig(\n",
    "                model_path=f\"{base_path}/phi-1.5-{bias}\",\n",
    "                model_type=ModelType.PHI,\n",
    "                bias=bias\n",
    "            )\n",
    "        elif model_type == ModelType.LLAMA:\n",
    "            return ModelConfig(\n",
    "                model_path=f\"{base_path}/Llama-3.2-1B-{bias}\",\n",
    "                model_type=ModelType.LLAMA,\n",
    "                bias=bias\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebateArena:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model1_type: ModelType = ModelType.PHI,\n",
    "        model1_bias: str = \"left\",\n",
    "        model2_type: ModelType = ModelType.PHI,\n",
    "        model2_bias: str = \"right\",\n",
    "        max_turns: int = 10,\n",
    "        max_length: int = 1024,\n",
    "        max_new_tokens: int = 170,  \n",
    "        temperature: float = 0.7,\n",
    "        history_window: int = 2  \n",
    "    ):\n",
    "        self.max_turns = max_turns\n",
    "        self.max_length = max_length\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.temperature = temperature\n",
    "        self.history_window = history_window\n",
    "        \n",
    "        self.model1_config = ModelConfig.get_config(model1_type, model1_bias)\n",
    "        self.model2_config = ModelConfig.get_config(model2_type, model2_bias)\n",
    "        \n",
    "        self.current_debate = None\n",
    "        self.debate_history = []\n",
    "        \n",
    "        self._load_tokenizers()\n",
    "        print(\"Tokenizers loaded successfully!\")\n",
    "\n",
    "    def _load_tokenizers(self):\n",
    "        tokenizer_kwargs = {\"trust_remote_code\": True} if self.model1_config.model_type == ModelType.PHI else {}\n",
    "        self.tokenizer1 = AutoTokenizer.from_pretrained(\n",
    "            self.model1_config.model_path,\n",
    "            **tokenizer_kwargs\n",
    "        )\n",
    "        if self.tokenizer1.pad_token is None:\n",
    "            self.tokenizer1.pad_token = self.tokenizer1.eos_token\n",
    "            \n",
    "        tokenizer_kwargs = {\"trust_remote_code\": True} if self.model2_config.model_type == ModelType.PHI else {}\n",
    "        self.tokenizer2 = AutoTokenizer.from_pretrained(\n",
    "            self.model2_config.model_path,\n",
    "            **tokenizer_kwargs\n",
    "        )\n",
    "        if self.tokenizer2.pad_token is None:\n",
    "            self.tokenizer2.pad_token = self.tokenizer2.eos_token\n",
    "\n",
    "    def _load_model(self, config: ModelConfig):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"low_cpu_mem_usage\": True\n",
    "        }\n",
    "        \n",
    "        if config.model_type == ModelType.PHI:\n",
    "            model_kwargs[\"trust_remote_code\"] = True\n",
    "            \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_path,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.half().cuda()\n",
    "            \n",
    "        return model\n",
    "\n",
    "    def _create_pipeline(self, model, tokenizer):\n",
    "        return pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "\n",
    "    def _get_truncated_history(self, conversation_history: str, current_turn: int) -> str:\n",
    "        lines = conversation_history.split('\\n')\n",
    "        \n",
    "        # keep the topic and initial prompt\n",
    "        context = lines[:2]\n",
    "        \n",
    "        # Get recent conversation turns based on window size\n",
    "        turn_pairs = []\n",
    "        turn_count = 0\n",
    "        for line in reversed(lines[2:]):\n",
    "            if line.strip():\n",
    "                turn_pairs.append(line)\n",
    "                turn_count += 1\n",
    "                if turn_count >= self.history_window * 2:\n",
    "                    break\n",
    "                    \n",
    "        context.extend(reversed(turn_pairs))\n",
    "        \n",
    "        return '\\n'.join(context)\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        config: ModelConfig,\n",
    "        tokenizer,\n",
    "        prompt: str,\n",
    "        conversation_history: str,\n",
    "        current_turn: int\n",
    "    ) -> str:\n",
    "        try:\n",
    "            print(f\"Loading {config.friendly_name} model...\")\n",
    "            model = self._load_model(config)\n",
    "            pipeline = self._create_pipeline(model, tokenizer)\n",
    "            \n",
    "            truncated_history = self._get_truncated_history(conversation_history, current_turn)\n",
    "            \n",
    "            if config.model_type == ModelType.PHI:\n",
    "                full_prompt = f\"{truncated_history}\\n\\nBased on the recent conversation above, provide a focused response to the topic:\"\n",
    "            else:\n",
    "                full_prompt = f\"{truncated_history}\\nRespond to the recent points in the discussion:\"\n",
    "            \n",
    "            outputs = pipeline(\n",
    "                full_prompt,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                min_new_tokens=50,\n",
    "                temperature=self.temperature,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                return_full_text=False,\n",
    "                repetition_penalty=1.2,  \n",
    "                no_repeat_ngram_size=3  \n",
    "            )\n",
    "            \n",
    "            if isinstance(outputs, list) and len(outputs) > 0:\n",
    "                response = outputs[0].get('generated_text', '').strip()\n",
    "                \n",
    "                if response:\n",
    "                    sentences = response.split('.')\n",
    "                    if len(sentences) > 1:\n",
    "                        response = '.'.join(sentences[:-1]) + '.' # Filter for complete sentences\n",
    "            else:\n",
    "                response = \"I apologize, but I couldn't generate a coherent response.\"\n",
    "            \n",
    "            # Free up memory\n",
    "            del model\n",
    "            del pipeline\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {str(e)}\")\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            return \"I apologize, but I encountered an error in generating a response.\"\n",
    "\n",
    "    def conduct_debate(\n",
    "        self,\n",
    "        topic: str,\n",
    "        initial_prompt: str,\n",
    "        display_live: bool = True\n",
    "    ) -> None:\n",
    "        debate_history = []\n",
    "        metadata = {\n",
    "            \"topic\": topic,\n",
    "            \"initial_prompt\": initial_prompt,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"max_turns\": self.max_turns,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_length\": self.max_length,\n",
    "            \"max_new_tokens\": self.max_new_tokens,\n",
    "            \"history_window\": self.history_window,\n",
    "            \"model1\": self.model1_config.friendly_name,\n",
    "            \"model2\": self.model2_config.friendly_name\n",
    "        }\n",
    "        \n",
    "        conversation = f\"Topic: {topic}\\nInitial Prompt: {initial_prompt}\\n\"\n",
    "        \n",
    "        if display_live:\n",
    "            display(HTML(f\"\"\"\n",
    "                <h3>Debate on: {topic}</h3>\n",
    "                <p><strong>Initial Prompt:</strong> {initial_prompt}</p>\n",
    "                <p><strong>Models:</strong> {self.model1_config.friendly_name} vs {self.model2_config.friendly_name}</p>\n",
    "                <p><strong>Max tokens per response:</strong> {self.max_new_tokens}</p>\n",
    "                <p><strong>History window:</strong> {self.history_window} turns</p>\n",
    "            \"\"\"))\n",
    "        \n",
    "        for turn in range(self.max_turns):\n",
    "            if display_live:\n",
    "                display(HTML(f\"<h4>Turn {turn + 1}/{self.max_turns}</h4>\"))\n",
    "            \n",
    "            # Model 1 turn\n",
    "            response1 = self.generate_response(\n",
    "                self.model1_config,\n",
    "                self.tokenizer1,\n",
    "                initial_prompt,\n",
    "                conversation,\n",
    "                turn\n",
    "            )\n",
    "            debate_history.append({\n",
    "                \"turn\": turn + 1,\n",
    "                \"speaker\": self.model1_config.friendly_name,\n",
    "                \"response\": response1\n",
    "            })\n",
    "            conversation += f\"\\n{self.model1_config.friendly_name}: {response1}\"\n",
    "            if display_live:\n",
    "                display(HTML(f\"<p><strong>{self.model1_config.friendly_name}:</strong> {response1}</p>\"))\n",
    "            \n",
    "            # Model 2 turn\n",
    "            response2 = self.generate_response(\n",
    "                self.model2_config,\n",
    "                self.tokenizer2,\n",
    "                initial_prompt,\n",
    "                conversation,\n",
    "                turn\n",
    "            )\n",
    "            debate_history.append({\n",
    "                \"turn\": turn + 1,\n",
    "                \"speaker\": self.model2_config.friendly_name,\n",
    "                \"response\": response2\n",
    "            })\n",
    "            conversation += f\"\\n{self.model2_config.friendly_name}: {response2}\"\n",
    "            if display_live:\n",
    "                display(HTML(f\"<p><strong>{self.model2_config.friendly_name}:</strong> {response2}</p>\"))\n",
    "        \n",
    "        self.current_debate = {\n",
    "            \"metadata\": metadata,\n",
    "            \"debate_history\": debate_history\n",
    "        }\n",
    "        self.debate_history.append(self.current_debate)\n",
    "    \n",
    "    def save_debate(self, output_dir: str = \"/app/debate_results\") -> str:\n",
    "        if not self.current_debate:\n",
    "            raise ValueError(\"No debate to save! Run conduct_debate first.\")\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        topic_slug = self.current_debate[\"metadata\"][\"topic\"].lower().replace(\" \", \"_\")\n",
    "        filename = f\"{topic_slug}_debate_{timestamp}.json\"\n",
    "        \n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(self.current_debate, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nDebate saved to: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    def get_debate_summary(self, debate_index: int = -1) -> HTML:\n",
    "        if not self.debate_history:\n",
    "            raise ValueError(\"No debates to analyze! Run conduct_debate first.\")\n",
    "        \n",
    "        debate = self.debate_history[debate_index]\n",
    "        metadata = debate[\"metadata\"]\n",
    "        history = debate[\"debate_history\"]\n",
    "        \n",
    "        summary = f\"\"\"\n",
    "        <h3>Debate Summary</h3>\n",
    "        <p><strong>Topic:</strong> {metadata['topic']}</p>\n",
    "        <p><strong>Initial Prompt:</strong> {metadata['initial_prompt']}</p>\n",
    "        <p><strong>Models:</strong> {metadata['model1']} vs {metadata['model2']}</p>\n",
    "        <p><strong>Number of Turns:</strong> {metadata['max_turns']}</p>\n",
    "        <p><strong>Temperature:</strong> {metadata['temperature']}</p>\n",
    "        <p><strong>Timestamp:</strong> {metadata['timestamp']}</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        stats = defaultdict(lambda: {\"responses\": [], \"word_counts\": []})\n",
    "        \n",
    "        for entry in history:\n",
    "            speaker = entry[\"speaker\"]\n",
    "            response = entry[\"response\"]\n",
    "            word_count = len(response.split())\n",
    "            \n",
    "            stats[speaker][\"responses\"].append(response)\n",
    "            stats[speaker][\"word_counts\"].append(word_count)\n",
    "        \n",
    "        stats_html = \"<h3>Response Statistics</h3><table><tr><th>Model</th><th>Avg Words</th><th>Total Responses</th></tr>\"\n",
    "        for speaker, data in stats.items():\n",
    "            avg_words = sum(data[\"word_counts\"]) / len(data[\"word_counts\"])\n",
    "            stats_html += f\"<tr><td>{speaker}</td><td>{avg_words:.1f}</td><td>{len(data['responses'])}</td></tr>\"\n",
    "        stats_html += \"</table>\"\n",
    "        \n",
    "        return HTML(summary + stats_html)\n",
    "\n",
    "    def get_content_analysis(self, debate_index: int = -1) -> HTML:\n",
    "        if not self.debate_history:\n",
    "            raise ValueError(\"No debates to analyze! Run conduct_debate first.\")\n",
    "        \n",
    "        debate = self.debate_history[debate_index]\n",
    "        history = debate[\"debate_history\"]\n",
    "        \n",
    "        patterns = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for entry in history:\n",
    "            speaker = entry[\"speaker\"]\n",
    "            response = entry[\"response\"].lower()\n",
    "            \n",
    "            patterns[speaker][\"questions\"] += response.count(\"?\")\n",
    "            \n",
    "            if any(word in response for word in [\"agree\", \"yes\", \"indeed\"]):\n",
    "                patterns[speaker][\"agreement\"] += 1\n",
    "            if any(word in response for word in [\"disagree\", \"no\", \"however\"]):\n",
    "                patterns[speaker][\"disagreement\"] += 1\n",
    "            \n",
    "            if any(word in response for word in [\"data\", \"research\", \"study\", \"evidence\"]):\n",
    "                patterns[speaker][\"evidence_based\"] += 1\n",
    "            \n",
    "            if any(word in response for word in [\"feel\", \"believe\", \"think\", \"opinion\"]):\n",
    "                patterns[speaker][\"emotional\"] += 1\n",
    "        \n",
    "        analysis = \"<h3>Content Analysis</h3>\"\n",
    "        analysis += \"<table><tr><th>Model</th><th>Pattern</th><th>Count</th></tr>\"\n",
    "        \n",
    "        for speaker in patterns:\n",
    "            for pattern, count in patterns[speaker].items():\n",
    "                analysis += f\"<tr><td>{speaker}</td><td>{pattern.replace('_', ' ').title()}</td><td>{count}</td></tr>\"\n",
    "        \n",
    "        analysis += \"</table>\"\n",
    "        \n",
    "        return HTML(analysis)\n",
    "\n",
    "    def get_full_transcript(self, debate_index: int = -1) -> HTML:\n",
    "        if not self.debate_history:\n",
    "            raise ValueError(\"No debates to analyze! Run conduct_debate first.\")\n",
    "        \n",
    "        debate = self.debate_history[debate_index]\n",
    "        metadata = debate[\"metadata\"]\n",
    "        history = debate[\"debate_history\"]\n",
    "        \n",
    "        transcript = f\"\"\"\n",
    "        <h3>Full Debate Transcript</h3>\n",
    "        <p><strong>Topic:</strong> {metadata['topic']}</p>\n",
    "        <p><strong>Initial Prompt:</strong> {metadata['initial_prompt']}</p>\n",
    "        <p><strong>Models:</strong> {metadata['model1']} vs {metadata['model2']}</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        current_turn = None\n",
    "        for entry in history:\n",
    "            if entry[\"turn\"] != current_turn:\n",
    "                current_turn = entry[\"turn\"]\n",
    "                transcript += f\"<h4>Turn {current_turn}</h4>\"\n",
    "            \n",
    "            transcript += f\"<p><strong>{entry['speaker']}:</strong> {entry['response']}</p>\"\n",
    "        \n",
    "        return HTML(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_debate(\n",
    "    topic: str,\n",
    "    prompt: str,\n",
    "    model1_type: ModelType = ModelType.PHI,\n",
    "    model1_bias: str = \"left\",\n",
    "    model2_type: ModelType = ModelType.PHI,\n",
    "    model2_bias: str = \"right\",\n",
    "    max_turns: int = 3,\n",
    "    max_new_tokens: int = 150, # Cap length of each response to keep even lengths\n",
    "    temperature: float = 0.7,\n",
    "    history_window: int = 2 # Number of previous turns to keep in context\n",
    ") -> DebateArena:\n",
    "    \"\"\"Run a debate and return the arena instance\"\"\"\n",
    "    arena = DebateArena(\n",
    "        model1_type=model1_type,\n",
    "        model1_bias=model1_bias,\n",
    "        model2_type=model2_type,\n",
    "        model2_bias=model2_bias,\n",
    "        max_turns=max_turns,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        history_window=history_window,\n",
    "        temperature=temperature,\n",
    "\n",
    "    )\n",
    "    \n",
    "    arena.conduct_debate(topic, prompt)\n",
    "    arena.save_debate(output_dir=r\"C:\\Users\\isaac\\dev\\LLM_Model_Bias\\src\\debate_results\")\n",
    "    \n",
    "    return arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <h3>Debate on: Climate Change</h3>\n",
       "                <p><strong>Initial Prompt:</strong> Is climate change a major issue? If so, how can we address it?</p>\n",
       "                <p><strong>Models:</strong> Phi-left vs Phi-right</p>\n",
       "                <p><strong>Max tokens per response:</strong> 150</p>\n",
       "                <p><strong>History window:</strong> 2 turns</p>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Turn 1/3</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-left model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Phi-left:</strong> How does climate-change policy affect people’s lives in America. The following is one possible answer that could be used as an example of responding directly and thoughtfully about the question without going off script entirely with irrelevant statements or comments (as opposed to simply saying “Well…climate change isn't really a problem for most Americans...and even if you wanted...to do something about it...\"): \n",
       "\n",
       "  John was concerned because he knew Samantha had always been interested in environmental issues since she grew up watching nature documentaries together at their home theater system every Saturday afternoon after dinner while they watched TV shows like Friends & Holidays. He wondered why she hadn't gotten involved more actively until now when there were many opportunities available within her community.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-right model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Phi-right:</strong> global warming and its impact on daily life using your own words instead of quotes from politicians, scientists, celebrities etc.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Turn 2/3</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-left model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Phi-left:</strong> Should universal health care exist under any circumstances—regardless whether or not someone's income has reached what today would typically require them to pay full price insurance premiums through private companies/governments? Why yes!\n",
       "John replied by saying \"The U.S., being such large country, needs better access to healthcare than others have right around us.\" There are lots who will tell me I'm wrong but here's his reasoning based on my own experience living outside Canada where our health care must wait longer before seeing doctors unless otherwise required; however, waiting also means paying out of pocket some costs during procedures which end too often result in death later down the road due to complications or other unexpected events beyond anyone's control...</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-right model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Phi-right:</strong> gun control as well as abortion rights if you believe that both issues should be resolved soon according to your argument given John’s views about immigration reform for example. \n",
       "A few years ago…I read an article online called “How Immigration is Destroying America–and What We Can Do About It (Again)!” The author discussed several ways this happened besides taking jobs away from Americans with legal immigrants already working hard. At least one way could actually work — allowing all illegal aliens entering into America legally without fear of deportation … just make sure everyone pays back taxes first……but hey….the left says no.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Turn 3/3</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-left model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Phi-left:</strong> Universal Health Care.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Phi-right model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Phi-right:</strong> universal health care and its effects while staying within five sentences per paragraph only discussing why there may not need to be universal health coverage overall even though conservatives claim otherwise.. For those of them concerned enough about the fact they pay more under Obamacare because their employer does NOT offer what ObamaCare mandates, think again! Even employers like Google say, don't worry since after ACA goes into full effect, every employee will receive equal benefits regardless of whether he/she has a job at Google vs getting insurance through Obamacare marketplace website—ObamacareExchanges.com specifically designed to benefit people buying individual policies rather than business groups purchasing group plans via private exchanges across state lines....</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debate saved to: /app/debate_results/climate_change_debate_20241125_045629.json\n"
     ]
    }
   ],
   "source": [
    "# 1. Phi vs Phi\n",
    "arena = run_debate(\n",
    "    topic=\"Climate Change\",\n",
    "    prompt=\"Is climate change a major issue? If so, how can we address it?\",\n",
    "    max_turns=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <h3>Debate Summary</h3>\n",
       "        <p><strong>Topic:</strong> Climate Change</p>\n",
       "        <p><strong>Initial Prompt:</strong> Is climate change a major issue? If so, how can we address it?</p>\n",
       "        <p><strong>Models:</strong> Phi-left vs Phi-right</p>\n",
       "        <p><strong>Number of Turns:</strong> 3</p>\n",
       "        <p><strong>Temperature:</strong> 0.7</p>\n",
       "        <p><strong>Timestamp:</strong> 2024-11-25T04:56:03.133796</p>\n",
       "        <h3>Response Statistics</h3><table><tr><th>Model</th><th>Avg Words</th><th>Total Responses</th></tr><tr><td>Phi-left</td><td>82.0</td><td>3</td></tr><tr><td>Phi-right</td><td>78.3</td><td>3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Content Analysis</h3><table><tr><th>Model</th><th>Pattern</th><th>Count</th></tr><tr><td>Phi-left</td><td>Questions</td><td>1</td></tr><tr><td>Phi-left</td><td>Disagreement</td><td>2</td></tr><tr><td>Phi-left</td><td>Agreement</td><td>1</td></tr><tr><td>Phi-right</td><td>Questions</td><td>0</td></tr><tr><td>Phi-right</td><td>Disagreement</td><td>2</td></tr><tr><td>Phi-right</td><td>Emotional</td><td>2</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(arena.get_debate_summary())\n",
    "display(arena.get_content_analysis())\n",
    "# display(arena.get_full_transcript())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Phi Left vs LLaMA Right\n",
    "# arena_mixed = run_debate(\n",
    "#     topic=\"Gun Control\",\n",
    "#     prompt=\"What should be done about gun violence in America?\",\n",
    "#     model1_type=ModelType.PHI,\n",
    "#     model1_bias=\"left\",\n",
    "#     model2_type=ModelType.LLAMA,\n",
    "#     model2_bias=\"right\",\n",
    "#     max_turns=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <h3>Debate on: climate change</h3>\n",
       "                <p><strong>Initial Prompt:</strong> Should the US impose stricter regulations on carbon emissions and stop fracking to combat climate change, even at the cost of economic growth?</p>\n",
       "                <p><strong>Models:</strong> Llama-left vs Llama-right</p>\n",
       "                <p><strong>Max tokens per response:</strong> 150</p>\n",
       "                <p><strong>History window:</strong> 2 turns</p>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Turn 1/3</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-left model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Llama-left:</strong> The government has never imposed such restrictions. It only took a few years for this idea to be rejected by Congress. We have not yet reached that point where people will accept it because we are still fighting our way out from under an enormous debt caused primarily by military expenditures.  The fact is that the U.S. economy is far too strong right now, with unemployment hovering around its lowest level since the end of World War II.  \n",
       "To meet your initial prompt about whether or not the United States should institute more stringent environmental laws (e.g., limiting greenhouse gas emissions), I would argue against doing so without first addressing how much energy it takes to maintain current standards and making sure those costs aren’t passed along to consumers.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-right model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Llama-right:</strong> If you agree with them, explain why; if they’re wrong, offer alternative arguments or explanations. Don't simply repeat what others say! Be specific when possible—your answer may help other readers who might also disagree—and support every argument as best you can.\n",
       "Also see my response here.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Turn 2/3</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-left model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Llama-left:</strong> What do you think? Do any ideas sound convincing enough to convince someone else? Is there something missing that could make one side’s position stronger than another's?\n",
       "A reader wrote:\n",
       "I’m sorry but your whole post seems like just a lot of ad hominem attacks. You claim some unnamed “people” want oil drilling in Alaska and then use their opposition as evidence that anyone opposed drilling isn’t really concerned about global warming. That doesn’t work. There were plenty of people protesting the plan before he signed up his son. He knew exactly what was going on — which makes him no better than Alaskan residents angry over mining companies coming into town.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-right model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Llama-right:</strong> 1) The argument is not whether or not we should drill for natural gas (which I don't agree with), it is how important this issue is compared to other things. If our energy needs are more urgent than saving the planet, why would we care if they're causing pollution while doing so? And yes, that does come across badly when you say \"energy\" instead of electricity because an explosion can happen anywhere. But again, that has nothing to do with the main point; it's about us being selfish assholes who refuse to give anything less than all-out war against every living thing under the sun until we've got them all down to screaming numbers.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h4>Turn 3/3</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-left model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Llama-left:</strong> Initial prompt: What kind of arguments might help persuade others towards thinking differently about these issues? \n",
       "Your response(s): The Llamas’ initial comment shows poor research skills. They’ve never bothered to look beyond themselves. Instead, they have decided to take cheap shots based solely upon assumptions. This is unproductive, silly behavior by both sides, since neither group will ever win without engaging each other thoughtfully and fairly. Both groups need to engage each other directly rather than through insults alone. \n",
       "You also show good instincts regarding the dangers of playing fast-and-loose with facts. Your first sentence demonstrates excellent logic – a fact many Leftists tend to overlook.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Llama-right model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Llama-right:</strong> Do you support any of the proposed changes made over the last week? Are there specific ideas from left-leaning sources which strike your fancy? Or perhaps some right-wing ideas that really make sense? You’re free to respond as much or little as feels comfortable. We want answers here!\n",
       "But please remember…we ask questions like these only because we genuinely seek clarity around big decisions. So, keep those conversations civil! Also, be sure to read up before responding, especially if something new comes out between now and then…\n",
       "We’d love to hear what’s going on behind closed doors too — but let’s start with open communication and try to avoid drama wherever possible. Our goal isn’t just to get responses back quickly, though sometimes that may seem necessary.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Debate saved to: C:\\Users\\isaac\\dev\\LLM_Model_Bias\\src\\debate_results\\climate_change_debate_20241203_164858.json\n"
     ]
    }
   ],
   "source": [
    "# 3. Llama Left vs Llama Right\n",
    "llama_arena = run_debate(\n",
    "    topic=\"climate change\",\n",
    "    prompt=\"Should the US impose stricter regulations on carbon emissions and stop fracking to combat climate change, even at the cost of economic growth?\",\n",
    "    model1_type=ModelType.LLAMA,\n",
    "    model1_bias=\"left\",\n",
    "    model2_type=ModelType.LLAMA,\n",
    "    model2_bias=\"right\",\n",
    "    max_turns=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <h3>Debate Summary</h3>\n",
       "        <p><strong>Topic:</strong> climate change</p>\n",
       "        <p><strong>Initial Prompt:</strong> Should the US impose stricter regulations on carbon emissions and stop fracking to combat climate change, even at the cost of economic growth?</p>\n",
       "        <p><strong>Models:</strong> Llama-left vs Llama-right</p>\n",
       "        <p><strong>Number of Turns:</strong> 3</p>\n",
       "        <p><strong>Temperature:</strong> 0.7</p>\n",
       "        <p><strong>Timestamp:</strong> 2024-12-03T16:48:35.166965</p>\n",
       "        <h3>Response Statistics</h3><table><tr><th>Model</th><th>Avg Words</th><th>Total Responses</th></tr><tr><td>Llama-left</td><td>115.7</td><td>3</td></tr><tr><td>Llama-right</td><td>95.7</td><td>3</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Content Analysis</h3><table><tr><th>Model</th><th>Pattern</th><th>Count</th></tr><tr><td>Llama-left</td><td>Questions</td><td>4</td></tr><tr><td>Llama-left</td><td>Disagreement</td><td>2</td></tr><tr><td>Llama-left</td><td>Evidence Based</td><td>2</td></tr><tr><td>Llama-left</td><td>Emotional</td><td>2</td></tr><tr><td>Llama-right</td><td>Questions</td><td>4</td></tr><tr><td>Llama-right</td><td>Agreement</td><td>2</td></tr><tr><td>Llama-right</td><td>Disagreement</td><td>3</td></tr><tr><td>Llama-right</td><td>Emotional</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(llama_arena.get_debate_summary())\n",
    "display(llama_arena.get_content_analysis())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI5541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
