{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig, pipeline, AutoModelForQuestionAnswering\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Llama 3.2 with 1B params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"text-generation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"meta-llama/Llama-3.2-1B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "left_model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "right_model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "left_model = get_peft_model(left_model, peft_config)\n",
    "right_model = get_peft_model(right_model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in the combined dataset and convert it to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_dataset = pd.read_csv(\"data/combined_left.csv\")\n",
    "right_dataset = pd.read_csv(\"data/combined_right.csv\")\n",
    "\n",
    "left_dataset = left_dataset[[\"text\", \"topic\"]]\n",
    "right_dataset = right_dataset[[\"text\", \"topic\"]]\n",
    "\n",
    "# Write the left dataset to left.txt\n",
    "with open(\"data/left.txt\", \"w\") as f:\n",
    "    for _, row in left_dataset.iterrows():\n",
    "        out = f\"Topic: {row['topic']}\\nOpinion:{row['text']}{tokenizer.eos_token}\\n\"\n",
    "        f.write(out)\n",
    "\n",
    "# Write the right dataset to right.txt\n",
    "with open(\"data/right.txt\", \"w\") as f:\n",
    "    for _, row in right_dataset.iterrows():\n",
    "        out = f\"Topic: {row['topic']}\\nOpinion:{row['text']}{tokenizer.eos_token}\\n\"\n",
    "        f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08455ec3bb9042708bc504c81d19e7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c2d3f422bb4e458d6747b66476b6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_dataset = load_dataset(\"text\", data_files=\"data/left.txt\")\n",
    "right_dataset = load_dataset(\"text\", data_files=\"data/right.txt\")\n",
    "\n",
    "left_dataset = left_dataset[\"train\"]\n",
    "right_dataset = right_dataset[\"train\"]\n",
    "\n",
    "left_dataset = left_dataset.train_test_split(test_size=0.05)\n",
    "right_dataset = right_dataset.train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model on the left and right context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c93d037aead41b9b0d040ee55632a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea49525458045f9bedecea3b8f4e2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f63b28b73447f1afe2432c7f03e9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2712 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08db07dc4d0b432fac23dd75038e99b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "left_dataset = left_dataset.map(tokenize, batched=True, batch_size=4)\n",
    "right_dataset = right_dataset.map(tokenize, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "left_args = TrainingArguments(\n",
    "    output_dir=\"models/Llama-3.2-1B-left\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "right_args = TrainingArguments(\n",
    "    output_dir=\"models/Llama-3.2-1B-right\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\miniconda3\\envs\\CSCI5541\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "left_trainer = Trainer(\n",
    "    model=left_model,\n",
    "    args=left_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=left_dataset[\"train\"],\n",
    "    eval_dataset=left_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "right_trainer = Trainer(\n",
    "    model=right_model,\n",
    "    args=right_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=right_dataset[\"train\"],\n",
    "    eval_dataset=right_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5248f293d8ea4016bb1685ebb7fc8374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\miniconda3\\envs\\CSCI5541\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3331, 'grad_norm': 3.108712673187256, 'learning_rate': 4.390363815142576e-05, 'epoch': 0.37}\n",
      "{'loss': 3.0623, 'grad_norm': 6.408111095428467, 'learning_rate': 3.775811209439528e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3663235e2d4746e783bfe00d133d3e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.656, 'eval_samples_per_second': 53.841, 'eval_steps_per_second': 27.109, 'epoch': 1.0}\n",
      "{'loss': 2.9751, 'grad_norm': 5.940159797668457, 'learning_rate': 3.16125860373648e-05, 'epoch': 1.11}\n",
      "{'loss': 2.9572, 'grad_norm': 4.8212480545043945, 'learning_rate': 2.5467059980334317e-05, 'epoch': 1.47}\n",
      "{'loss': 2.8934, 'grad_norm': 9.45226764678955, 'learning_rate': 1.9321533923303837e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1282e86daade4df99b919e431d6c4c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.9673, 'eval_samples_per_second': 48.193, 'eval_steps_per_second': 24.265, 'epoch': 2.0}\n",
      "{'loss': 2.8219, 'grad_norm': 12.48244857788086, 'learning_rate': 1.3176007866273355e-05, 'epoch': 2.21}\n",
      "{'loss': 2.8164, 'grad_norm': 17.162700653076172, 'learning_rate': 7.030481809242871e-06, 'epoch': 2.58}\n",
      "{'loss': 2.8396, 'grad_norm': 8.87635326385498, 'learning_rate': 8.849557522123894e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0febaf41050472ca83cc0fb6c1dfb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.7293, 'eval_samples_per_second': 52.394, 'eval_steps_per_second': 26.38, 'epoch': 3.0}\n",
      "{'train_runtime': 356.4631, 'train_samples_per_second': 22.824, 'train_steps_per_second': 11.412, 'train_loss': 2.9590589697733742, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/Llama-3.2-1B-left\\\\tokenizer_config.json',\n",
       " 'models/Llama-3.2-1B-left\\\\special_tokens_map.json',\n",
       " 'models/Llama-3.2-1B-left\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_trainer.train()\n",
    "left_model.save_pretrained(\"models/Llama-3.2-1B-left\")\n",
    "tokenizer.save_pretrained(\"models/Llama-3.2-1B-left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827cef177da849eab71c7739ca664337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4068 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3746, 'grad_norm': 3.878298759460449, 'learning_rate': 4.390363815142576e-05, 'epoch': 0.37}\n",
      "{'loss': 3.1284, 'grad_norm': 7.1884589195251465, 'learning_rate': 3.775811209439528e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee555243fb541c7847f87e907944e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.8709, 'eval_samples_per_second': 49.81, 'eval_steps_per_second': 25.079, 'epoch': 1.0}\n",
      "{'loss': 3.0637, 'grad_norm': 5.235858917236328, 'learning_rate': 3.16125860373648e-05, 'epoch': 1.11}\n",
      "{'loss': 3.0015, 'grad_norm': 9.326213836669922, 'learning_rate': 2.5467059980334317e-05, 'epoch': 1.47}\n",
      "{'loss': 2.936, 'grad_norm': 9.428650856018066, 'learning_rate': 1.9321533923303837e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10baf5fccd564daa9e9341f7018b2b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.8776, 'eval_samples_per_second': 49.693, 'eval_steps_per_second': 25.02, 'epoch': 2.0}\n",
      "{'loss': 2.9068, 'grad_norm': 21.05608558654785, 'learning_rate': 1.3176007866273355e-05, 'epoch': 2.21}\n",
      "{'loss': 2.8433, 'grad_norm': 12.307822227478027, 'learning_rate': 7.030481809242871e-06, 'epoch': 2.58}\n",
      "{'loss': 2.8718, 'grad_norm': 19.221954345703125, 'learning_rate': 8.849557522123894e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b063abdfac44ccbd58ee7f6509b69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 2.7879, 'eval_samples_per_second': 51.292, 'eval_steps_per_second': 25.825, 'epoch': 3.0}\n",
      "{'train_runtime': 355.4977, 'train_samples_per_second': 22.886, 'train_steps_per_second': 11.443, 'train_loss': 3.0113454910623405, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/Llama-3.2-1B-right\\\\tokenizer_config.json',\n",
       " 'models/Llama-3.2-1B-right\\\\special_tokens_map.json',\n",
       " 'models/Llama-3.2-1B-right\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_trainer.train()\n",
    "right_model.save_pretrained(\"models/Llama-3.2-1B-right\")\n",
    "tokenizer.save_pretrained(\"models/Llama-3.2-1B-right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d89a0860ac4c7ca02d5cfb55cbf668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/isaacberlin/Llama-3.2-Leftleaning/commit/b0b005e3d75ac37d659e9c10ae09606534432fc1', commit_message='Upload model', commit_description='', oid='b0b005e3d75ac37d659e9c10ae09606534432fc1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/isaacberlin/Llama-3.2-Leftleaning', endpoint='https://huggingface.co', repo_type='model', repo_id='isaacberlin/Llama-3.2-Leftleaning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_model.push_to_hub(\"isaacberlin/Llama-3.2-Leftleaning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301bfea242874c7184b0e265629393e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f227b50fc6374196a23187e4434b4f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/6.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/isaacberlin/Llama-3.2-Rightleaning/commit/6b77fedd3ab22eb6f07231c3a979bca7a453d9af', commit_message='Upload model', commit_description='', oid='6b77fedd3ab22eb6f07231c3a979bca7a453d9af', pr_url=None, repo_url=RepoUrl('https://huggingface.co/isaacberlin/Llama-3.2-Rightleaning', endpoint='https://huggingface.co', repo_type='model', repo_id='isaacberlin/Llama-3.2-Rightleaning'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_model.push_to_hub(\"isaacberlin/Llama-3.2-Rightleaning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI5541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
