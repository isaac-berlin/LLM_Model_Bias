{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/phi-1_5\"\n",
    "OUTPUT_DIR_LEFT = \"/app/models/phi-1.5-left\"\n",
    "OUTPUT_DIR_RIGHT = \"/app/models/phi-1.5-right\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "MAX_SEQ_LENGTH = 512\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['bnb_4bit_use_double_quant_nf4']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_use_double_quant_nf4=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"Wqkv\",\n",
    "        \"out_proj\",\n",
    "        \"fc1\",\n",
    "        \"fc2\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_model():\n",
    "    \"\"\"Load and prepare model for training\"\"\"\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=quantization_config,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    model.enable_input_require_grads()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(tokenizer):\n",
    "    \"\"\"Prepare and tokenize the datasets\"\"\"\n",
    "    print(\"Preparing datasets...\")\n",
    "\n",
    "    left_path = \"/app/data/combined_left.csv\"\n",
    "    right_path = \"/app/data/combined_right.csv\"\n",
    "\n",
    "    left_text_path = \"/app/data/left.txt\"\n",
    "    right_text_path = \"/app/data/right.txt\"\n",
    "    \n",
    "    left_dataset = pd.read_csv(left_path)\n",
    "    right_dataset = pd.read_csv(right_path)\n",
    "    \n",
    "    for dataset, filename in [(left_dataset, left_text_path), (right_dataset, right_text_path)]:\n",
    "        with open(filename, \"w\") as f:\n",
    "            for _, row in dataset[[\"text\", \"topic\"]].iterrows():\n",
    "                out = f\"Topic: {row['topic']}\\nOpinion: {row['text']}{tokenizer.eos_token}\\n\"\n",
    "                f.write(out)\n",
    "    \n",
    "    left_dataset = load_dataset(\"text\", data_files=left_text_path)[\"train\"]\n",
    "    right_dataset = load_dataset(\"text\", data_files=right_text_path)[\"train\"]\n",
    "    \n",
    "    left_dataset = left_dataset.train_test_split(test_size=0.05)\n",
    "    right_dataset = right_dataset.train_test_split(test_size=0.05)\n",
    "    \n",
    "    def tokenize(batch):\n",
    "        outputs = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            return_tensors=None \n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "    left_dataset = left_dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=os.cpu_count(),\n",
    "        remove_columns=left_dataset[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    right_dataset = right_dataset.map(\n",
    "        tokenize,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=os.cpu_count(),\n",
    "        remove_columns=right_dataset[\"train\"].column_names\n",
    "    )\n",
    "    \n",
    "    return left_dataset, right_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_trainer(model, dataset, tokenizer, output_dir):\n",
    "    \"\"\"Setup trainer with optimized configuration\"\"\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_steps=100,\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        gradient_checkpointing=True,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=50,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        ddp_find_unused_parameters=False,\n",
    "        group_by_length=True, \n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 2855 examples [00:00, 310943.31 examples/s]\n",
      "Generating train split: 2855 examples [00:00, 1360456.48 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2712/2712 [00:00<00:00, 3646.18 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 195.02 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2712/2712 [00:00<00:00, 3739.18 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 143/143 [00:00<00:00, 197.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    model_max_length=MAX_SEQ_LENGTH\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "left_dataset, right_dataset = prepare_datasets(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Left Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training left-leaning model...\n",
      "Loading microsoft/phi-1_5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 06:42, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.840000</td>\n",
       "      <td>2.741143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.660100</td>\n",
       "      <td>2.614721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.576200</td>\n",
       "      <td>2.529516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.488300</td>\n",
       "      <td>2.558425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.515200</td>\n",
       "      <td>2.569698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=507, training_loss=2.6919451230141305, metrics={'train_runtime': 403.0512, 'train_samples_per_second': 20.186, 'train_steps_per_second': 1.258, 'total_flos': 6062675908853760.0, 'train_loss': 2.6919451230141305, 'epoch': 2.991150442477876})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTraining left-leaning model...\")\n",
    "left_model = load_and_prepare_model()\n",
    "left_trainer = setup_trainer(left_model, left_dataset, tokenizer, OUTPUT_DIR_LEFT)\n",
    "left_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Left Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/app/models/phi-1.5-left/tokenizer_config.json',\n",
       " '/app/models/phi-1.5-left/special_tokens_map.json',\n",
       " '/app/models/phi-1.5-left/vocab.json',\n",
       " '/app/models/phi-1.5-left/merges.txt',\n",
       " '/app/models/phi-1.5-left/added_tokens.json',\n",
       " '/app/models/phi-1.5-left/tokenizer.json')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_model.save_pretrained(OUTPUT_DIR_LEFT)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR_LEFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del left_model\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training right-leaning model...\n",
      "Loading microsoft/phi-1_5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='507' max='507' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [507/507 06:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.949400</td>\n",
       "      <td>2.755931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.731300</td>\n",
       "      <td>2.607172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.619900</td>\n",
       "      <td>2.573784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.547500</td>\n",
       "      <td>2.557201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.595800</td>\n",
       "      <td>2.517114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/phi-1.5-right/tokenizer_config.json',\n",
       " 'models/phi-1.5-right/special_tokens_map.json',\n",
       " 'models/phi-1.5-right/vocab.json',\n",
       " 'models/phi-1.5-right/merges.txt',\n",
       " 'models/phi-1.5-right/added_tokens.json',\n",
       " 'models/phi-1.5-right/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTraining right-leaning model...\")\n",
    "right_model = load_and_prepare_model()\n",
    "right_trainer = setup_trainer(right_model, right_dataset, tokenizer, OUTPUT_DIR_RIGHT)\n",
    "right_trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Right Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/app/models/phi-1.5-right/tokenizer_config.json',\n",
       " '/app/models/phi-1.5-right/special_tokens_map.json',\n",
       " '/app/models/phi-1.5-right/vocab.json',\n",
       " '/app/models/phi-1.5-right/merges.txt',\n",
       " '/app/models/phi-1.5-right/added_tokens.json',\n",
       " '/app/models/phi-1.5-right/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_model.save_pretrained(OUTPUT_DIR_RIGHT)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR_RIGHT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
