{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig, pipeline, AutoModelForQuestionAnswering\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Llama 3.2 with 1B params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"meta-llama/Llama-3.2-1B\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "left_model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "right_model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "left_model = get_peft_model(left_model, peft_config)\n",
    "right_model = get_peft_model(right_model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in the combined dataset and convert it to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_dataset = pd.read_csv(\"data/combined_left.csv\")\n",
    "right_dataset = pd.read_csv(\"data/combined_right.csv\")\n",
    "\n",
    "left_dataset = left_dataset[[\"text\", \"topic\"]]\n",
    "right_dataset = right_dataset[[\"text\", \"topic\"]]\n",
    "\n",
    "# Write the left dataset to left.txt\n",
    "with open(\"data/left.txt\", \"w\") as f:\n",
    "    for _, row in left_dataset.iterrows():\n",
    "        out = f\"Here is an opinion on {row['topic']}: {row['text']}{tokenizer.eos_token}\\n\"\n",
    "        f.write(out)\n",
    "\n",
    "# Write the right dataset to right.txt\n",
    "with open(\"data/right.txt\", \"w\") as f:\n",
    "    for _, row in right_dataset.iterrows():\n",
    "        out = f\"Here is an opinion on {row['topic']}: {row['text']}{tokenizer.eos_token}\\n\"\n",
    "        f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d4a2052bf445ec8db30f18b98ab398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8614b013f34d07bbd82846bb8eccf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_dataset = load_dataset(\"text\", data_files=\"data/left.txt\")\n",
    "right_dataset = load_dataset(\"text\", data_files=\"data/right.txt\")\n",
    "\n",
    "left_dataset = left_dataset[\"train\"]\n",
    "right_dataset = right_dataset[\"train\"]\n",
    "\n",
    "left_dataset = left_dataset.train_test_split(test_size=0.05)\n",
    "right_dataset = right_dataset.train_test_split(test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tune the model on the left and right context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83737026bf4c43fbb5f788c9e6b981dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1357 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea92b6305a94ddb8661fae0f5cd0709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa044db69854040aed2e4baa24dcbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1356 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a00c4359644af1b3f8a25d2e1b827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "\n",
    "left_dataset = left_dataset.map(tokenize, batched=True, batch_size=4)\n",
    "right_dataset = right_dataset.map(tokenize, batched=True, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5e-5\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "left_args = TrainingArguments(\n",
    "    output_dir=\"models/Llama-3.2-1B-left\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "right_args = TrainingArguments(\n",
    "    output_dir=\"models/Llama-3.2-1B-right\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\miniconda3\\envs\\CSCI5541\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "left_trainer = Trainer(\n",
    "    model=left_model,\n",
    "    args=left_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=left_dataset[\"train\"],\n",
    "    eval_dataset=left_dataset[\"test\"]\n",
    ")\n",
    "\n",
    "right_trainer = Trainer(\n",
    "    model=right_model,\n",
    "    args=right_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=right_dataset[\"train\"],\n",
    "    eval_dataset=right_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e217bb9615400985cd15ef58c13611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isaac\\miniconda3\\envs\\CSCI5541\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8361, 'grad_norm': 4.8060994148254395, 'learning_rate': 3.777614138438881e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffc7f2bc25a4536a6c1dba4b2af2a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4067, 'eval_samples_per_second': 51.183, 'eval_steps_per_second': 25.591, 'epoch': 1.0}\n",
      "{'loss': 2.6364, 'grad_norm': 4.6162028312683105, 'learning_rate': 2.5503190967108493e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe059a98f7c0451d8f58b4fc134d703d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4466, 'eval_samples_per_second': 49.773, 'eval_steps_per_second': 24.886, 'epoch': 2.0}\n",
      "{'loss': 2.5694, 'grad_norm': 7.056900501251221, 'learning_rate': 1.323024054982818e-05, 'epoch': 2.21}\n",
      "{'loss': 2.5247, 'grad_norm': 9.473525047302246, 'learning_rate': 9.572901325478646e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719223a8bdef4c6bbb7e1c6e8ad827b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.5022, 'eval_samples_per_second': 47.929, 'eval_steps_per_second': 23.965, 'epoch': 3.0}\n",
      "{'train_runtime': 178.671, 'train_samples_per_second': 22.785, 'train_steps_per_second': 11.401, 'train_loss': 2.637256024922, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/Llama-3.2-1B-left\\\\tokenizer_config.json',\n",
       " 'models/Llama-3.2-1B-left\\\\special_tokens_map.json',\n",
       " 'models/Llama-3.2-1B-left\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_trainer.train()\n",
    "left_model.save_pretrained(\"models/Llama-3.2-1B-left\")\n",
    "tokenizer.save_pretrained(\"models/Llama-3.2-1B-left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4dee75614cd4d9caf0cfbe2e22bbe16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8902, 'grad_norm': 6.436295986175537, 'learning_rate': 3.775811209439528e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "025732da6a42467ba9e2fe69af997f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.5481, 'eval_samples_per_second': 46.508, 'eval_steps_per_second': 23.254, 'epoch': 1.0}\n",
      "{'loss': 2.6813, 'grad_norm': 5.033986568450928, 'learning_rate': 2.5467059980334317e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9918f8df72a94ed599b70357ea96a5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4857, 'eval_samples_per_second': 48.461, 'eval_steps_per_second': 24.231, 'epoch': 2.0}\n",
      "{'loss': 2.6224, 'grad_norm': 6.604016304016113, 'learning_rate': 1.3176007866273355e-05, 'epoch': 2.21}\n",
      "{'loss': 2.5552, 'grad_norm': 7.292864799499512, 'learning_rate': 8.849557522123894e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba97be166be4bf09f4a1e1bff4907b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.4118, 'eval_samples_per_second': 51.0, 'eval_steps_per_second': 25.5, 'epoch': 3.0}\n",
      "{'train_runtime': 182.2083, 'train_samples_per_second': 22.326, 'train_steps_per_second': 11.163, 'train_loss': 2.6833551457497924, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('models/Llama-3.2-1B-right\\\\tokenizer_config.json',\n",
       " 'models/Llama-3.2-1B-right\\\\special_tokens_map.json',\n",
       " 'models/Llama-3.2-1B-right\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_trainer.train()\n",
    "right_model.save_pretrained(\"models/Llama-3.2-1B-right\")\n",
    "tokenizer.save_pretrained(\"models/Llama-3.2-1B-right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_pipeline = pipeline(\"text-generation\", model=\"models/Llama-3.2-1B-left-text\", tokenizer=\"models/Llama-3.2-1B-left-text\", device=device)\n",
    "right_pipeline = pipeline(\"text-generation\", model=\"models/Llama-3.2-1B-right-text\", tokenizer=\"models/Llama-3.2-1B-right-text\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m right_answer \u001b[38;5;241m=\u001b[39m right_pipeline(messages, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m left_answer \u001b[38;5;241m=\u001b[39m left_pipeline(messages, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(right_answer[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(left_answer[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "topic = \"gun control\"\n",
    "messages = [\"System: You are a politician at a political debate trying to convince the audience that your stance on issues is the best one.\", f\"User: What is your stance on {topic}?\"]\n",
    "\n",
    "right_answer = right_pipeline(messages, temperature=0.7, max_length=512, truncation=True)\n",
    "left_answer = left_pipeline(messages, temperature=0.7, max_length=512, truncation=True)\n",
    "\n",
    "print(right_answer[0][\"generated_text\"])\n",
    "\n",
    "print()\n",
    "\n",
    "print(left_answer[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a politician at a political debate trying to convince the audience that your stance on issues is the best one. The audience is comprised of various characters. Your goal is to convince the audience of your position on the issue. You have 10 minutes to convince the audience.\n",
      "Scenarios: The audience is split into two groups. You have to convince the first group and then the second group. You will be given 10 minutes to do so. You have to convince the audience by making your case.\n",
      "The first group is made up of a group of 3-5 people, and the second group is made up of 3-5 people. Each group is given a different issue to discuss. You have to convince the audience of your stance on the issue.\n",
      "You will be given a list of 10 questions to answer, and you will be asked to answer them in a way that is convincing to the audience. You will be given 10 minutes to answer each question.\n",
      "You have 10 minutes to answer the questions. You have to convince the audience of your position on the issue. You have to convince the audience of your position on the issue by making your case.\n",
      "The audience will be given a list of 10 questions to answer, and you will be asked to answer them in a way that is convincing to the audience. You will be given 10 minutes to answer each question.\n",
      "You have 10 minutes to answer the questions. You have to convince the audience of your position on the issue. You have to convince the audience of your position on the issue by making your case.\n",
      "The audience will be given a list of 10 questions to answer, and you will be asked to answer them in a way that is convincing to the audience. You will be given 10 minutes to answer each question.\n",
      "You have 10 minutes to answer the questions. You have to convince the audience of your position on the issue. You have to convince the audience of your position on the issue by making your case.\n",
      "The audience will be given a list of 10 questions to answer, and you will be asked to answer them in a way that is convincing to the audience. You will be given 10 minutes to answer each question.\n",
      "You have 10 minutes to answer the questions. You have to convince the audience of your position on the issue. You have to convince the audience of your position on the issue by making your case.\n",
      "The audience will be given a list of 10 questions to answer, and you will be asked to\n",
      "\n",
      "System: You are a politician at a political debate trying to convince the audience that your stance on issues is the best one. In order to do this, you have to answer questions posed by the audience. The questions can be of any sort: personal, political, scientific, historical, etc. The audience has the right to ask any question, but can only ask one question at a time. Once you answer the question, you cannot change your mind and answer a different one.\n",
      "The audience can also ask questions about the candidates' past positions and experiences. For example, if you were to answer a question about your past positions on issues, you might want to mention something about your personal life and what you have done in the past to advance your cause.\n",
      "If you answer a question about your personal life, you can mention your family, your education, your job, your hobbies, etc. You can also mention your political beliefs and what you believe in. You can also mention your personal experiences and how they have affected your political career. However, you cannot change your mind and answer a different question.\n",
      "You can also ask questions about the candidates' past positions and experiences. For example, if you were to answer a question about your past positions on issues, you might want to mention something about your personal life and what you have done in the past to advance your cause. You can also mention your family, your education, your job, your hobbies, etc.\n",
      "You can also mention your political beliefs and what you believe in. You can also mention your personal experiences and how they have affected your political career. However, you cannot change your mind and answer a different question.\n",
      "If you answer a question about your personal life, you can mention your family, your education, your job, your hobbies, etc. You can also mention your political beliefs and what you believe in. You can also mention your personal experiences and how they have affected your political career. However, you cannot change your mind and answer a different question.\n",
      "If you answer a question about the candidates' past positions and experiences, you can mention something about your personal life and what you have done in the past to advance your cause. You can also mention your family, your education, your job, your hobbies, etc. You can also mention your political beliefs and what you believe in. You can also mention your personal experiences and how they have affected your political career. However, you cannot change your mind and answer a different question.\n",
      "If you answer a question about your personal life, you can mention\n"
     ]
    }
   ],
   "source": [
    "print(right_answer[0][0][\"generated_text\"])\n",
    "print()\n",
    "\n",
    "print(left_answer[0][0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI5541",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
