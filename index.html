<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: &#39;Lato&#39;, sans-serif;">Exploring Bias in Different LLM Models and its Discrepancies to Human Judgment</h1>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: &#39;Lato&#39;, sans-serif; ">Auto “Encoder” Bots</h4>

      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="files\img\Jerome_Newhouse_Headshot.png" alt="Jerome Newhouse Headshot">
            
            
          </div>
          <p>
                        
            Jerome Newhouse
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="files\img\Isaac_Berlin_Headshot.png" alt="Isaac Berlin Headshot">
            
          </div>
          <p>
            
            Isaac Berlin
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="files/img/Kyle_Anthes_Headshot.jpg" alt="Kyle_Anthes_Headshot">            
            
          </div>
          <p>
              Kyle Anthes
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
                        
              <img src="files/img/Y_Utku_Alcalar_Headshot.jpg" alt="Y. Utku Alcalar Headshot">
            
          </div>
          <p>
            Y. Utku Alcalar
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href="https://github.com/isaac-berlin/LLM_Model_Bias"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>

    <b>NOTE: </b> <p>Because our project has changed recently the information in this website is partially from our previous project proposal. Please see the Future Work section for more information about our new project.</p>
    <h2 id="abstract">Abstract</h2>

<p>This study investigates potential political bias
  in distinct large language models (LLMs) by
  examining their ability to classify biased content using the Bias Annotations By Experts
  (BABE) dataset. Previous work has focused
  on bias in news outlets, but with the rising influence of LLMs in research and information
  retrieval, understanding their political leanings
  is critical. We task these models with classifying politically biased sentences and compare
  their predictions against ground truth labels.
  By analyzing how accurately the models detect bias across the political spectrum, we assess whether they perform differently for leftleaning versus right-leaning content. Discrepancies in performance could reveal inherent
  biases in the models themselves. Specifically,
  our research aims to shed light on the political
  neutrality of widely-used LLMs.</p>

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
We aim to investigate the political bias of large language models (LLMs) by examining their ability to classify biased content.
We source this content both from the BABE dataset and from manually labeled news articles relating to the 2024 Election in the United States.
Our main goal is understanding how well these models can detect bias in text, and whether they perform differently for left-leaning versus right-leaning content.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
It has laready been proven that LLMs can be biased, but previously conducted studies have mainly focused on either generating text or answering questions.
These methods are good generalizations, but they do not provide enough information about the models' ability to detect bias in novel text.
<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
If we are successful, we will be able to shed light on the political neutrality of a few SOTA LLMs. 
This will assist in understanding the potential biases in these models when dealing with breaking news.
Overall, it will help us categorize the bias of these models and potentially improve them in the future.
</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
We used a finetuned a few SOTA LLMs to detect bias in text from the BABE dataset and from manually labeled news articles. 
This allows us to have a baseline for how well these models can detect and classify bias. 
It also allows us to see how these models will perform when the facts are not as clear as they are in the BABE dataset.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
We were optimistic that the models would be able to detect bias in the BABE dataset, but were eventually met with subpar results. 
Our first pass at training the models did not work as expected, and we have decided to move forward with a different approach.
</p>

<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>
We measured success by comparing the models' predictions to the ground truth labels in the BABE dataset. We unfortunately did not succeed in detecting bias in the BABE dataset, but we are hopeful that our new approach will yield better results.
</p>
<table>
  <thead>
    <tr>
      <th style="text-align: center"><strong>Model</strong></th>
      <th style="text-align: center">Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">DistilBERT</td>
      <td style="text-align: center">0.4839</td>
    </tr>
    <tr>
      <td style="text-align: center">RoBERTa</td>
      <td style="text-align: center">0.554</td>
    </tr>
    <tr>
      <td style="text-align: center">BERT</td>
      <td style="text-align: center">0.5177</td>
    </tr>
    <tr>
      <td style="text-align: center">ALBERT</td>
      <td style="text-align: center">0.5640</td>
    </tr>
  </tbody>
  <caption>Table 1. Models and their errors on the BABE dataset</caption>
</table>

<br>
<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

In the future we are transitioning to a new project. We recieved feedback that our current research is not novel enough and have decided to switch to a multi agent bias conversation project.
We are going to train left leaning, right leaning, and neutral agents to have a conversation. These models will be prompted with persuasive techniques and will be evaluated on how well they can persuade the other agent.
These models will be trained with biased text from the BABE dataset and from manually labeled news articles. We will then evaluate how well these models can persuade the other agents based on human metrics.
We plan to pit the left leaning and right leaning agents against each other to see how well they can persuade each other. We will also evaluate how well the left and right leaning agents can persuade the neutral model.

<hr>


  </div>
  


</body></html>
